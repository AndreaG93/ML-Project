@unknown{Falessi,
author = {Falessi, Davide and Narayana, Likhita and Thai, Jennifer and Turhan, Burak},
year = {2018},
month = {09},
pages = {},
title = {Preserving Order of Data When Validating Defect Prediction Models}
}

@InProceedings{Normalization,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Sergey Ioffe and Christian Szegedy},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/ioffe15.html},
}

@article{WALK1,
author = {Cao, L.J. and Tay, Francis},
year = {2003},
month = {12},
pages = {1506 - 1518},
title = {Support vector machine with adaptive parameters in financial time series forecasting},
volume = {14},
journal = {Neural Networks, IEEE Transactions on},
doi = {10.1109/TNN.2003.820556}
}

@article{WALK2,
title = "Designing a neural network for forecasting financial and economic time series",
journal = "Neurocomputing",
volume = "10",
number = "3",
pages = "215 - 236",
year = "1996",
note = "Financial Applications, Part II",
issn = "0925-2312",
doi = "https://doi.org/10.1016/0925-2312(95)00039-9",
url = "http://www.sciencedirect.com/science/article/pii/0925231295000399",
author = "Iebeling Kaastra and Milton Boyd",
keywords = "Neural networks, Financial forecasting, Backpropagation, Data preprocessing, Training, Testing, Validation, Network paradigms, Learning rate, Momentum and forecast evaluation criteria",
abstract = "Artificial neural networks are universal and highly flexible function approximators first used in the fields of cognitive science and engineering. In recent years, neural network applications in finance for such tasks as pattern recognition, classification, and time series forecasting have dramatically increased. However, the large number of parameters that must be selected to develop a neural network forecasting model have meant that the design process still involves much trial and error. The objective of this paper is to provide a practical introductory guide in the design of a neural network for forecasting economic time series data. An eight-step procedure to design a neural network forecasting model is explained including a discussion of tradeoffs in parameter selection, some common pitfalls, and points of disagreement among practitioners."
}

@inproceedings{NILMOTHER,
author = {Kelly, Jack and Knottenbelt, William},
year = {2015},
month = {11},
pages = {},
title = {Neural NILM: Deep Neural Networks Applied to Energy Disaggregation},
doi = {10.1145/2821650.2821672}
}

@article{meaninpiu,
author = {Schirmer, Pascal and Mporas, Iosif},
year = {2019},
month = {06},
pages = {3222},
title = {Statistical and Electrical Features Evaluation for Electrical Appliances Energy Disaggregation},
volume = {11},
journal = {Sustainability},
doi = {10.3390/su11113222}
}

@book{FalessiDataMining,
author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A. and Pal, Christopher J.},
title = {Data Mining, Fourth Edition: Practical Machine Learning Tools and Techniques},
year = {2016},
isbn = {0128042915},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {4th},
abstract = {Data Mining: Practical Machine Learning Tools and Techniques, Fourth Edition, offers a thorough grounding in machine learning concepts, along with practical advice on applying these tools and techniques in real-world data mining situations. This highly anticipated fourth edition of the most acclaimed work on data mining and machine learning teaches readers everything they need to know to get going, from preparing inputs, interpreting outputs, evaluating results, to the algorithmic methods at the heart of successful data mining approaches. Extensive updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including substantial new chapters on probabilistic methods and on deep learning. Accompanying the book is a new version of the popular WEKA machine learning software from the University of Waikato. Authors Witten, Frank, Hall, and Pal include today's techniques coupled with the methods at the leading edge of contemporary research. Please visit the book companion website at http://www.cs.waikato.ac.nz/ml/weka/book.html It contains Powerpoint slides for Chapters 1-12. This is a very comprehensive teaching resource, with many PPT slides covering each chapter of the bookOnline Appendix on the Weka workbench; again a very comprehensive learning aid for the open source software that goes with the book.}
}

@book{DeepKeras,
author = {Gulli, Antonio and Pal, Sujit},
title = {Deep Learning with Keras},
year = {2017},
isbn = {1787128423},
publisher = {Packt Publishing},
abstract = {Key Features Implement various deep-learning algorithms in Keras and see how deep-learning can be used in games See how various deep-learning models and practical use-cases can be implemented using Keras A practical, hands-on guide with real-world examples to give you a strong foundation in Keras Book Description This book starts by introducing you to supervised learning algorithms such as simple linear regression, the classical multilayer perceptron and more sophisticated deep convolutional networks. You will also explore image processing with recognition of hand written digit images, classification of images into different categories, and advanced objects recognition with related image annotations. An example of identification of salient points for face detection is also provided. Next you will be introduced to Recurrent Networks, which are optimized for processing sequence data such as text, audio or time series. Following that, you will learn about unsupervised learning algorithms such as Autoencoders and the very popular Generative Adversarial Networks (GAN). You will also explore non-traditional uses of neural networks as Style Transfer. Finally, you will look at Reinforcement Learning and its application to AI game playing, another popular direction of research and application of neural networks. What you will learn Optimize step-by-step functions on a large neural network using the Backpropagation Algorithm Fine-tune a neural network to improve the quality of results Use deep learning for image and audio processing Use Recursive Neural Tensor Networks (RNTNs) to outperform standard word embedding in special cases Identify problems for which Recurrent Neural Network (RNN) solutions are suitable Explore the process required to implement Autoencoders Evolve a deep neural network using reinforcement learning About the Author Antonio Gulli is a software executive and business leader with a passion for establishing and managing global technological talent, innovation, and execution. He is an expert in search engines, online services, machine learning, information retrieval, analytics, and cloud computing. So far, he has been lucky enough to gain professional experience in four different countries in Europe and managed people in six different countries in Europe and America. Antonio served as CEO, GM, CTO, VP, director, and site lead in multiple fields spanning from publishing (Elsevier) to consumer internet (Ask.com and Tiscali) and high-tech R&D (Microsoft and Google). Sujit Pal is a technology research director at Elsevier Labs, working on building intelligent systems around research content and metadata. His primary interests are information retrieval, ontologies, natural language processing, machine learning, and distributed processing. He is currently working on image classification and similarity using deep learning models. Prior to this, he worked in the consumer healthcare industry, where he helped build ontology-backed semantic search, contextual advertising, and EMR data processing platforms. He writes about technology on his blog at Salmon Run.}
}


